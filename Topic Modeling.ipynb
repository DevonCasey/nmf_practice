{
 "metadata": {
  "name": "",
  "signature": "sha256:29d4618d3eda04a87c94fc3681f6cd4990936c21c58fbe5d9afc26f18e82db00"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Topic Modeling Overview\n",
      "==============================================\n",
      "\n",
      "Relation to Dimensionality Reduction\n",
      "================================================\n",
      "\n",
      "Topic Modeling uses dimensionality reduction techniques to model and group the data in meaningful ways (by word occurrences)\n",
      "\n",
      "In this lecture, we will be treating these dimensionality reduction techniques as roughly the same.\n",
      "\n",
      "\n",
      "Why are we doing this?\n",
      "\n",
      "If we remember PCA/SVD from yesterday, these 2 things are both dimensionality reduction techniques.\n",
      "\n",
      "SVD's D (right single matrix) is equivalent to principal components scaled by zero mean and unit variance.\n",
      "\n",
      "\n",
      "\n",
      "Motivating Example\n",
      "=============================================\n",
      "\n",
      "Topic Modeling is modeling the joint distribution of words with documents and topics. Specify a number of topics, and words/documents will be clustered in to different topics. The more topics you specify, the more finegrained the categories become.\n",
      "\n",
      "Topic modeling is for identifying the content of a corpus and providing a way of automatically grouping content in meaningful ways. This allows you to do everything from summarize content based on relevance to a topic, sentiment analysis based on topic (is the topic positive or negative?) among any number of things. You can also search by topic.\n",
      "\n",
      "One typical use case is grouping news stories to identify what's relevant for readers.\n",
      "\n",
      "\n",
      "Reuters Newsgroup Visualization with PCA\n",
      "\n",
      "![alt text](images/topicmodeling.png \"Topic Modeling\")\n",
      "\n",
      "\n",
      "\n",
      "Likelihood of a topic being in a given topic in a given document. Think of this simplex as being able to pinpoint WHERE a given word falls within the word space within the topic space.\n",
      "\n",
      "![alt text](images/topicmodel-simplex.png \"Topic Modeling\")\n",
      "\n",
      "\n",
      "\n",
      "Mathematically: What is it?\n",
      "=====================================================\n",
      "\n",
      "As mentioned earlier, it's modeling the joint distribution over words, documents, and latent topics. This is in general an unsupervised way of grouping documents relative to their word occurrences.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How does this tie in to what we did previously?\n",
      "=======================================================================\n",
      "\n",
      "\n",
      "You can run the same NLP pipeline (TFIDF vectorizer ring a bell?) and use those bag of words vectors to automatically cluster documents in to topics.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NMF (Non Negative Matrix Factorization\n",
      "=============================================\n",
      "\n",
      "\n",
      "Tiein with SVD. Very similar to SVD where you have a left matrix U (representing row values) and a right matrix V (representing column values), NMF is another form of decomposition for non negative matrices. The basic idea with SVD and doing a reconstruction was the following:\n",
      "\n",
      "To approximate the data you did:\n",
      "\n",
      "u[1,k] * diag(d[1,k] * v[1,k]T\n",
      "\n",
      "to do a reconstruction.\n",
      "\n",
      "NMF also has a similar concept of reconstructions.\n",
      "\n",
      "\n",
      "Conceptual Overview\n",
      "==============================================\n",
      "\n",
      "\n",
      "[NMF](http://en.wikipedia.org/wiki/Non-negative_matrix_factorization) is a form of matrix decomposition (think SVD) that takes an  m x n matrix (of all numbers being >= 0) and decomposes it down to  2 matrices W and H of dimensions m x r and r x n respectively where r is <= min(m,n). . NMF generates factors with significantly reduced dimensions compared to the original matrix. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The property then holds that: \n",
      "\n",
      "W * H = V\n",
      "\n",
      "W are the principal components row wise. H are principal components column wise.\n",
      "\n",
      "\n",
      "A quick example\n",
      "\n",
      "W =\n",
      "\n",
      "   0.91860   0.73835\n",
      "   0.26502   0.65643\n",
      "   0.93525   0.96078\n",
      "   0.48499   0.10956\n",
      "\n",
      "H =\n",
      "\n",
      "   0.780323   0.048058   0.460162   0.956932   0.935853   0.807745\n",
      "\n",
      "   0.531417   0.494166   0.627717   0.940814   0.685886   0.301090\n",
      "\n",
      "V = W * H = \n",
      "\n",
      "   0.248891   0.050057   0.170430   0.326982   0.302153   0.238898\n",
      "   \n",
      "   0.401785   0.039895   0.247256   0.502213   0.483461   0.407730\n",
      "   \n",
      "   0.935341   0.294726   0.713106   1.295608   1.146717   0.840256\n",
      "   \n",
      "   0.794035   0.500370   0.775793   1.256624   0.999800   0.578318\n",
      "\n",
      "\n",
      "\n",
      "We fine these decompositions via numerically solving for them via loss functions (think SGD). \n",
      "\n",
      "Some successful algorithms are based on alternating non-negative least squares: in each step of such an algorithm, first H is fixed and W found by a non-negative least squares solver, then W is fixed and H is found analogously. The procedures used to solve for W and H may be the same[16] or different, as some NMF variants regularize one of W and H.[14] Specific approaches include the projected gradient descent methods,[16][17] the active set method,[3][18] and the block principal pivoting method[19] among several others.\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models.ldamodel import LdaModel\n",
      "from gensim import corpora\n",
      "from gensim import matutils\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from sklearn import linear_model\n",
      "import numpy as np\n",
      "newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "vec = CountVectorizer(min_df=10, stop_words='english')\n",
      "\n",
      "X = vec.fit_transform(newsgroups_train.data)\n",
      "vocab = vec.get_feature_names()\n",
      "\n",
      "def fit_classifier(X, y, C=0.1):\n",
      "    \"\"\" Fit L1 Logistic Regression classifier. \"\"\"\n",
      "    # Smaller C means fewer features selected.\n",
      "    clf = linear_model.LogisticRegression(penalty='l1', C=C)\n",
      "    clf.fit(X, y)\n",
      "    return clf\n",
      " \n",
      "def fit_lda(X, vocab, num_topics=5, passes=20):\n",
      "    \"\"\" Fit LDA from a scipy CSR matrix (X). \"\"\"\n",
      "    print 'fitting lda...'\n",
      "    return LdaModel(matutils.Sparse2Corpus(X), num_topics=num_topics,\n",
      "                    passes=passes,\n",
      "                    id2word=dict([(i, s) for i, s in enumerate(vocab)]))\n",
      "\n",
      "def print_topics(lda, vocab, n=10):\n",
      "    \"\"\" Print the top words for each topic. \"\"\"\n",
      "    topics = lda.show_topics(topics=-1, topn=n, formatted=False)\n",
      "    for ti, topic in enumerate(topics):\n",
      "        print 'topic %d: %s' % (ti, ' '.join('%s/%.2f' % (t[1], t[0]) for t in topic))\n",
      "\n",
      "def print_features(clf, vocab, n=10):\n",
      "    \"\"\" Print sorted list of non-zero features/weights. \"\"\"\n",
      "    coef = clf.coef_[0]\n",
      "    print 'positive features: %s' % (' '.join(['%s/%.2f' % (vocab[j], coef[j]) for j in np.argsort(coef)[::-1][:n] if coef[j] > 0]))\n",
      "    print 'negative features: %s' % (' '.join(['%s/%.2f' % (vocab[j], coef[j]) for j in np.argsort(coef)[:n] if coef[j] < 0]))\n",
      "\n",
      "\n",
      "# Fit classifier.\n",
      "clf = fit_classifier(X, newsgroups_train.target)\n",
      "print_features(clf, vocab)\n",
      "\n",
      "# Fit LDA.\n",
      "lda = fit_lda(X, vocab)\n",
      "print_topics(lda, vocab)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "positive features: benedikt/1.52 atheism/1.50 okcforum/1.32 bmd/1.25 wwc/1.15 keith/1.10 jaeger/1.02 wingate/0.98 livesey/0.92 atheists/0.91\n",
        "negative features: organization/-0.50 usa/-0.43 mail/-0.40 rutgers/-0.39 ca/-0.35 thanks/-0.35 use/-0.31 line/-0.27 year/-0.25 10/-0.25\n",
        "fitting lda...\n",
        "topic 0: acknowledged/0.01 gnp/0.01 700/0.01 herman/0.01 confirmed/0.00 colorado/0.00 nigel/0.00 minutes/0.00 nodomain/0.00 ehrlich/0.00"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "topic 1: nist/0.03 monninger/0.02 churchill/0.01 lookout/0.01 longer/0.01 cabell/0.01 french/0.01 clay/0.01 mutual/0.00 concerned/0.00\n",
        "topic 2: cuts/0.02 homers/0.02 continually/0.01 agnostic/0.01 kawasaki/0.01 hangs/0.01 differing/0.01 frequent/0.01 ke/0.01 csd4/0.01\n",
        "topic 3: discourse/0.10 military/0.09 mag/0.09 delta/0.08 demos/0.07 157/0.07 alarm/0.07 credibility/0.06 chassis/0.05 canadians/0.04\n",
        "topic 4: mock/0.01 hull/0.00 fw/0.00 institute/0.00 qy/0.00 dont/0.00 9mm/0.00 fallacy/0.00 alleged/0.00 davet/0.00\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "LDA\n",
      "=====================================\n",
      "\n",
      "LDA is a generative model over a set of documents.\n",
      "\n",
      "Documents are represented as a mixture over latent topics\n",
      "wherein each topic is categorized over a distinct set of words (remember your vocab?)\n",
      "\n",
      "\n",
      "We can represent this probability distribution as follows:\n",
      "\n",
      "![alt text](images/lda-probability.png \"LDA Probability\")\n",
      "\n",
      "\n",
      "Let theta represent the topic mixture over a set of documents over a set of N topics z,\n",
      "and a set of N words w.\n",
      "\n",
      "\n",
      "If we remember the simplex from earlier, think of LDA as a way of placing points on that triangle. The \"allocation\" part of the algorithm places the words in a 3 nested bin.\n",
      "\n",
      "\n",
      "This can be represented with a simplex. Given a 3 edge simple with points A,B, and C, let's demonstrate with a basically Dirchlet tries to find.\n",
      "\n",
      "The basic idea is we have a triangle with 3 points, and we want to try to model where the document falls over a set of topics given its words.\n",
      "\n",
      "![alt text](images/density-unigrams.png \"LDA Probability\")\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}