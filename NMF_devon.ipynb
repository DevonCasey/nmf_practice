{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy.linalg import lstsq\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import NMF as skNMF\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMF(object):\n",
    "    def __init__(self, V, latent_topics, max_iter=20, error=0.01):\n",
    "        \"\"\"\n",
    "        Initialize our weights (W) and features (H) matrices\n",
    "        Initialize the weights matrix (W) with (positive) random values to be a n x k matrix, where n is the number of documents and k is the number of latent topics\n",
    "        Initialize the feature matrix (H) to be k x m where m is the number of words in our vocabulary (i.e. length of bag).\n",
    "        \"\"\"\n",
    "        self.V = V\n",
    "        self.k = latent_topics\n",
    "        self.max_iter = max_iter\n",
    "        self.W = np.random.choice(1000, size=[self.V.shape[0], self.k])\n",
    "        self.H = np.random.choice(1000, size=[self.k, self.V.shape[1]])\n",
    "        self.error = error\n",
    "\n",
    "    def update_H(self):\n",
    "            \"\"\"\n",
    "            minimizing the sum of squared errors predicting the document matrix\n",
    "            \"\"\"\n",
    "            self.H = lstsq(self.W, self.V)[0]\n",
    "            self.H[self.H<0] = 0\n",
    "\n",
    "    def update_W(self):\n",
    "            \"\"\"\n",
    "            Use the same lstsq solver to update W while holding H fixed\n",
    "            \"\"\"\n",
    "            W_T = lstsq(self.H.T, self.V.T)[0]\n",
    "            self.W = W_T.T\n",
    "            self.W[self.W<0] = 0\n",
    "\n",
    "    def fit(self):\n",
    "            \"\"\"\n",
    "            Repeat update_H and update_W for max_iter iterations, or until convergence (change in cost(V, W*H) close to 0)\n",
    "            \"\"\"\n",
    "            for i in range(self.max_iter):\n",
    "                if self.cost() > self.error:\n",
    "                    print(\"Iteration %d | Current cost: %f\" % (i, self.cost()))\n",
    "                    self.update_H()\n",
    "                    self.update_W()\n",
    "                else:\n",
    "                    print(\"Convergence met.\")\n",
    "\n",
    "    def cost(self):\n",
    "        \"\"\"\n",
    "        cost function is mean squared error\n",
    "        \"\"\"\n",
    "        return mean_squared_error(self.V, np.dot(self.W, self.H))\n",
    "\n",
    "    def key_feat_idx(self):\n",
    "        keys = np.argsort(self.H)\n",
    "        return keys\n",
    "\n",
    "    def load_data(file=\"data/articles.pkl\", X_col='content', y_col='section_name'):\n",
    "        \"\"\"\n",
    "        Load, tokenize and vectorize data\n",
    "        \"\"\"\n",
    "        df = pd.read_pickle(file)\n",
    "        X = df[X_col]\n",
    "        y = df[y_col]\n",
    "        vect = CountVectorizer(max_features=5000, stop_words='english')\n",
    "        tok = vect.fit_transform(X)\n",
    "        return df, X, y, tok, vect\n",
    "\n",
    "    def main(self):\n",
    "        \"\"\"\n",
    "        Test function\n",
    "        \"\"\"\n",
    "        print(\"Loading data and initilizing...\")\n",
    "        df, X, y, tok, vect = load_data()\n",
    "        feature_name = np.array(vect.get_feature_names())\n",
    "        section_names = df['section_name'].unique()\n",
    "        print(\"Running NMF...\")\n",
    "        nmf = NMF(tok.todense(), len(section_names))\n",
    "        nmf.fit()\n",
    "        print(\"Getting top topics...\")\n",
    "        top_features = feature_name[np.argsort(nmf.H)]\n",
    "        top_five_features = top_features[:, :-6:-1]\n",
    "        print(\"Top five features (Sorted by Topics from 0-10:\")\n",
    "        print(top_five_features)\n",
    "        tfdif = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "        sk_tok = tfdif.fit_transform(X)\n",
    "        print(\"Total cost of model: %f\" % (nmf.cost()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and initilizing...\n",
      "Running NMF...\n",
      "Iteration 0 | Current cost: 6835987975171.811523\n",
      "Iteration 1 | Current cost: 0.677821\n",
      "Iteration 2 | Current cost: 0.621696\n",
      "Iteration 3 | Current cost: 0.599254\n",
      "Iteration 4 | Current cost: 0.587945\n",
      "Iteration 5 | Current cost: 0.582109\n",
      "Iteration 6 | Current cost: 0.578941\n",
      "Iteration 7 | Current cost: 0.576557\n",
      "Iteration 8 | Current cost: 0.574301\n",
      "Iteration 9 | Current cost: 0.571780\n",
      "Iteration 10 | Current cost: 0.569075\n",
      "Iteration 11 | Current cost: 0.566950\n",
      "Iteration 12 | Current cost: 0.565152\n",
      "Iteration 13 | Current cost: 0.563807\n",
      "Iteration 14 | Current cost: 0.562906\n",
      "Iteration 15 | Current cost: 0.562348\n",
      "Iteration 16 | Current cost: 0.561972\n",
      "Iteration 17 | Current cost: 0.561716\n",
      "Iteration 18 | Current cost: 0.561554\n",
      "Iteration 19 | Current cost: 0.561456\n",
      "Getting top topics...\n",
      "Top five features (Sorted by Topics from 0-10:\n",
      "[['new' 'work' 'company' 'like' 'year']\n",
      " ['davis' 'state' 'story' 'texas' 'woman']\n",
      " ['said' 'year' 'company' 'percent' 'day']\n",
      " ['iran' 'mr' 'rouhani' 'nuclear' 'obama']\n",
      " ['mr' 'said' 'party' 'year' 'case']\n",
      " ['team' 'game' 'season' 'yard' 'point']\n",
      " ['government' 'united' 'state' 'syria' 'country']\n",
      " ['gun' 'child' 'firearm' 'year' 'death']\n",
      " ['republican' 'house' 'government' 'health' 'law']\n",
      " ['game' 'yankee' 'rivera' 'season' 'run']]\n",
      "Total cost of model: 0.561399\n",
      "List of keys: \n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'H' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-c692bece2bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'H' is not defined"
     ]
    }
   ],
   "source": [
    "np.argsort(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
